Burp Suite Other modules: 
    * Focusing on lesser known DECODER, COMPARER, SEQUENCER AND ORGANISER.
    * They help operations with encoded text, comparison of data sets, analysis of randomness of tokens, store and annotate HTTP responses for later.

    DECODER:
        * Manipulation of data, allows decode and encode. Create hashsums. Smart decode does it's best to return to plaintext.
        * In decoder have a main window where text is placed for enc/dec. Either text/hex display. Drop down gives options for enc/dec/hash.
        - URL Encoding:         Ensures safe transmission of data in URL request. Substitutes ASCII codes in hex preceded by % for chars. e.g / --> 47 --> %2f (hex).
        - HTML Encoding:        Replaces special chars with & + hex number/ reference to char + ; Ensures safe rendering of HTML and prevents XSS.
        - Gzip Encoding:        Compresses data, reducing file load times. Desirable for devs ++ load time, SEO.
        - Others:               Base64, Hex, Octal, Binary, ASCII hex.

        * Hex view is good for byte-by-byte input editing. Vital when working with non ASCII/ Binary.
        * Smart decode tries to auto-decode

        HASHING:
            * For a function to qualify as a hashing algorithm, the output it generates must be irreversible
            * Every data input creates a unique hash!
            * Commonly used to verify file integrity due to any changes being relected drastically in the hashsum.
            * Used to store passwords due to the one way process. Hashed password stored in database, application takes user input and hashes it, compares.
            * Decoder allows us to create hashsums with the hash drop down.
            N.B! Hashed output is not pure ASCIIs! Therefore customary to convert to ASCII hex string after hash.
    
    COMPARER:
        * Allows us to compare two sets of data, either by ASCII or by bytes.
        * Left side is items to compare, top right gives options to insert data. Bottom right compare by words or bytes.
        * Bottom left shows the copmarison key, coloured highlights the modifications/ deletions/ additions.
        * Sync views ensures both sets of data are represented the same (hex/ text views).

        - Typical use:  Capture request, send to repeater, send. Then right click response and send to comparer.
                        Edit request in repeater, sent to comparer. Compare either byte or words.

    SEQUENCER:
        * Analyses the randomness or entropy of tokens. Tokens are strings used to identify something, ideally generated in cryptographically secure manner.
        * Could be session cookies, or CSRF (Cross Site Request Forgery) used to protect form submissions.
        
        Sequencer has two methods for token analysis:

        * Live capture:
            - Default/common method. Lets us pass a request that will generate a token  to sequencer for analysis.
            - Example:  want to pass a POST request to login endpoint, send to sequencer. We know server will respond with cookie. 
                        Then, instruct sequencer to start a live capture. Thousands of requests made, cookies stored, entropy analysis.
            
            - Manual load: Load a list of pre-generated tokens directly into sequencer for analysis.

        Example: Entropy analysis for http://10.10.63.57/admin/login/.
            * Capture request, send to sequencer. Within Token Location... can select between cookie, form field (token) or custom. Start live capture.
            * Allow approx. 10k requests, then analyse now. Auto analyse option gives periodic updates.

            - Report produced has four sections:

                Overall result:         This gives a broad assessment of the security of the token generation mechanism. In this case, the level of entropy indicates that the tokens are likely securely generated.
                Effective entropy:      This measures the randomness of the tokens. The effective entropy of 117 bits is relatively high, indicating that the tokens are sufficiently random and, therefore, secure against prediction or brute force attacks.
                Reliability:            The significance level of 1% implies that there is 99% confidence in the accuracy of the results. This level of confidence is quite high, providing assurance in the accuracy of the effective entropy estimation.
                Sample:                 This provides details about the token samples analyzed during the entropy testing process, including the number of tokens and their characteristics.

        **An effective entropy of 117 bits and significance level of 1% suggests robustly secure token generation process.
    

    ORGANISER:
        - Helps store and annotate copies of http requests that may be revisited later.
        - Good for organising a pen testing workflow.
        - Send to organiser using ctr + O.
        - Each HTTP request sent here is read only copy of original from when it was sent to Organiser.
        - Requests are stored in a table. Columns of info about the request.
        
        * To view any request & corresponding response:
            - Click on an item, use search to find within request/ response.
        
    SUMMARY:
    - Decoder allows decode/ encode/ hashing of data. Easier to read, understand and convert data to appropriate format.
    - Comparer allows spotting of differences between data. Good for identifying vulns or anomalies.
    - Sequencer allows entropy analysis of tokens. Gives insight on randomness & therefore security level.
    - Organiser allows storage and annotation of HTTP requests for later study.